{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataIntegrationOutline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6LcZRg592S_"
      },
      "source": [
        "#Install Java, Spark, Findspark, and PySpark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxVHIWam98RT",
        "outputId": "0dccdd51-f1f5-464d-8584-c92ce9dc389a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-04 23:34:52--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  1.40MB/s    in 0.6s    \n",
            "\n",
            "2020-10-04 23:34:54 (1.40 MB/s) - ‘postgresql-42.2.9.jar’ saved [914037/914037]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fai2HbkL-IQR"
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameFunctions\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rUU8ObW-Njf",
        "outputId": "df906283-0e21-411e-a4b0-a6e2852f65fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "# url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"hotel_bookings.csv\"), inferSchema=True, sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1b243762653c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# url =\"https://s3.amazonaws.com/dataviz-curriculum/day_1/wine.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkFiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hotel_bookings.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Show DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/tmp/spark-a36db67f-2ebf-4805-ba90-2389dc7851a9/userFiles-51b23f7d-e09b-409e-b8d8-b7e21e33d7f4/hotel_bookings.csv;"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpPvWyKadeBc"
      },
      "source": [
        "# Drop null values \n",
        "df = df.dropna()\n",
        "\n",
        "print(df.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J3T9-DpiT6p"
      },
      "source": [
        "# Print schema \n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYZveSvhiZmU"
      },
      "source": [
        "# Change datatypes \n",
        "from pyspark.sql.types import VarcharType, IntegerType, BooleanType\n",
        "\n",
        "df = df.withColumn(\"bookings_business_info\", df[\"bookings_business_info\"].cast(IntegerType()))\\\n",
        "       .withColumn(\"bookings_guest_info\", df[\"bookings_guest_info\"].cast(IntegerType()))\\\n",
        "       .withColumn(\"bookings_schedule_info\", df[\"bookings_schedule_info\"].cast(IntegerType()))\\\n",
        "       .withColumn(\"guests\", df[\"guests\"].cast(BooleanType()))\\\n",
        "       .withColumn(\"hotel\", df[\"hotel\"].cast(IntegerType()))\\\n",
        "     \n",
        "\n",
        "# Print schema\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLJvv7pnkCi4"
      },
      "source": [
        "# Create bookings_business_info dataframe to match bookings_business_info table\n",
        "bookings_business_info_df = df.select([\"deposit_type\",\"agent\",\"company\", \"market_segment\",\"distribution_channel\"])\n",
        "bookings_business_info_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jc0XdPOl_Px"
      },
      "source": [
        "# Print schema\n",
        "bookings_business_info_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG5TzilNrVkN"
      },
      "source": [
        "# Create customer dataframe to match customers table\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "customers_df = df.groupby(\"customer_id\").agg({\"customer_id\":\"count\"})\n",
        "customers_df = customers_df.orderBy(desc(\"count(customer_id)\"))\n",
        "customers_df = customers_df.withColumnRenamed(\"count(customer_id)\", \"customer_count\") \n",
        "customers_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfmuJo8jmMh3"
      },
      "source": [
        "# Create bookings_guest_info dataframe to match bookings_business_info table\n",
        "bookings_guest_info_df = df.select([\"adults\",\n",
        "                                    \"children\",\n",
        "                                    \"babies\", \n",
        "                                    \"meal\",\n",
        "                                    \"required_car_parking_spaces\", \n",
        "                                    \"total_of_special_requests\", \n",
        "                                    \"reservation_status\",\n",
        "                                    \"reservation_status_date\",\n",
        "                                    ])\n",
        "bookings_guest_info_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc5fHeu5m_Py"
      },
      "source": [
        "# Print schema\n",
        "bookings_guest_info_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3rRwTManCdz"
      },
      "source": [
        "# Create bookings_schedule_info dataframe to match bookings_schedule_info table\n",
        "bookings_schedule_info_df = df.select([\"is_canceled\",\n",
        "                                    \"lead_time\",\n",
        "                                    \"arrival_date_year\", \n",
        "                                    \"arrival_date_month\",\n",
        "                                    \"arrival_date_week_number\", \n",
        "                                    \"arrival_date_day_of_month\", \n",
        "                                    \"stays_in_weekend_nights\",\n",
        "                                    \"stays_in_week_nights\",\n",
        "                                    \"reserved_room_type\",\n",
        "                                    \"assigned_room_type\",\n",
        "                                    \"booking_changes\"\n",
        "                                    ])\n",
        "bookings_schedule_info_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLKO2Y0TnMwM"
      },
      "source": [
        "  # Print schema\n",
        "bookings_schedule_info_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpk8vPoKo0BY"
      },
      "source": [
        "# Create guests dataframe to match guests table\n",
        "guests_df = df.select([\"country\",\n",
        "                      \"customer_type\",\n",
        "                      \"is_repeated_guest\", \n",
        "                      \"days_in_waiting_list\",\n",
        "                      \"previous_cancellations\", \n",
        "                      \"previous_bookings_not_canceled\"\n",
        "                        ])\n",
        "guests_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyMniigKqovc"
      },
      "source": [
        "  # Print schema\n",
        "guests_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTS4EdJEqrGt"
      },
      "source": [
        "# Create hotel dataframe to match hotel table\n",
        "hotels_df = df.select([\"hotel\", \"adr\"])\n",
        "                      \n",
        "hotel_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iBdI2s8rD-i"
      },
      "source": [
        "  # Print schema\n",
        "hotel_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFoC4T_3s0A-"
      },
      "source": [
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"amazonawsurl\"\n",
        "config = {\"user\":\"xx\", \n",
        "          \"password\": \"xx\", \n",
        "          \"driver\":\"xx\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d-3mCI6s_eM",
        "outputId": "ec80bc53-7594-460c-ef64-8e7f22496201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Load dependencies\n",
        "!pip install psycopg2-binary\n",
        "import psycopg2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting psycopg2-binary\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/1b/720b36697158113ca1b2221a8e96a470088ccf3770d182214689d1a96a07/psycopg2_binary-2.8.6-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 8.0MB/s \n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.8.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKSDYnRRtFLy"
      },
      "source": [
        "# Connect to Postgres db\n",
        "try:\n",
        "    connection = psycopg2.connect(user = \"xxx\",\n",
        "                                  password = \"xxx\",\n",
        "                                  host = \"xxx\",\n",
        "                                  port = \"xxx\",\n",
        "                                  database = \"xxx\")\n",
        "\n",
        "    cursor = connection.cursor()\n",
        "    # Print PostgreSQL Connection properties\n",
        "    print ( connection.get_dsn_parameters(),\"\\n\")\n",
        "\n",
        "    # Print PostgreSQL version\n",
        "    cursor.execute(\"SELECT version();\")\n",
        "    record = cursor.fetchone()\n",
        "    print(\"You are connected to - \", record,\"\\n\")\n",
        "\n",
        "except (Exception, psycopg2.Error) as error :\n",
        "    print (\"Error while connecting to PostgreSQL\", error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9sQK4DItVHM"
      },
      "source": [
        "   \n",
        "   postgreSQL_select_Query = \"select * from xx where xx\n",
        "\n",
        "   cursor.execute(postgreSQL_select_Query)\n",
        "   print(\"Selecting rows from hotel_bookings using cursor.fetchall\")\n",
        "   hotel_bookings = cursor.fetchall() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edSCCitSuclQ"
      },
      "source": [
        "# analysis section "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz8qW1y2ugxS"
      },
      "source": [
        "# Close Postgres connection\n",
        "try:\n",
        "    connection = psycopg2.connect(user = \"xx\",\n",
        "                                  password = \"xx\",\n",
        "                                  host = \"xx\",\n",
        "                                  port = \"xx\",\n",
        "                                  database = \"xx\")\n",
        "\n",
        "finally:\n",
        "    #closing database connection.\n",
        "        if(connection):\n",
        "            cursor.close()\n",
        "            connection.close()\n",
        "            print(\"PostgreSQL connection is closed\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}